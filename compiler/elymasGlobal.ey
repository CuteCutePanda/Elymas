0 /TOKINT defv
1 /TOKSTR defv
2 /TOKID defv

{ /f deff _1 /s defv regex { f } { s } ? * } /rxparse deff

{ " " cat
  { < /type defv /value defv > } /token deff
  [ -01 { _ "" streq not } {
    0 /any defv { /f deff any { - } { { 1 =any f } rxparse } ? * } /parse deff

    "^ (.*)" { } parse
    "^#" { "" } parse
    "^(\\d+) +(.*)" { TOKINT token -01 } parse
    "\"(.*)" {
      "" /str defv
      { _ "^\"(.*)" regex { --0 0 } { 1 } ? * } {
        0 /strany defv { /f deff strany { - } { { 1 =strany f } rxparse } ? * } /strparse deff

        "^\\\\\\\\(.*)" { str "\\" cat =str } strparse
        "^\\\\n(.*)" { str "\n" cat =str } strparse
        "^\\\\\"(.*)" { str "\"" cat =str } strparse
        "^([^\"\\\\])(.*)" { str -01 cat =str } strparse
        strany not { "Tokenization of string-liked failed" die } rep
      } loop
      str TOKSTR token -01
    } parse
    "^([^a-zA-Z ]+)([a-zA-Z]+) +(.*)" { -201 TOKSTR token " " -1203 cat cat } parse
    "^([a-zA-Z]+|[^a-zA-Z ]+) +(.*)" { TOKID token -01 } parse
    any not { "Tokenization failed" die } rep
  } loop - ]
} /tokenize deff

{
  dump
} /interpretToken deff

{ /input defv
  "" {
    4096 input .readstr cat
    _ "" streq not
  } {
    { _ "([^\\n]*)\\n(.*)" regex } { ---10 tokenize |interpretToken each } loop
  } loop
} /executeFile deff

# vim: syn=elymas
